---
title: "p8105_hw3_xh2636"
author: "Xiaoyu Huang"
date: "2023-10-13"
output: github_document
---

# Problem 1
```{r}
library(p8105.datasets)
library(tidyverse)
library(ggplot2)
data("instacart")
```

```{r, include=FALSE}
data_dim <- dim(instacart)

# Print the number of observations and variables
num_obs <- nrow(instacart)
num_obs
num_var <- ncol(instacart)
num_var
```

The dataset has `r num_obs` observation and `r num_var` variables. The key variable in the dataset are their “ids”, “aisle”, and “department name”. Also, we can see the data set contain different product name.

```{r, include=FALSE}
# Find number of aisle
num_aisles <- length(unique(instacart$aisle))
num_aisles
```

```{r}
# Find number aisles are the most items ordered from and rank it
rank_aisle <- instacart %>%
  group_by(aisle) %>%
  summarize(total_ordered = n())%>%
  arrange(-total_ordered)
rank_aisle
```

The are `r num_aisles` aisles, and aisle have the most items ordered from is fresh vegetable.

```{r}
# limiting this to aisles with more than 10000 items ordered.
aisle_counts <- table(instacart$aisle)
popular_aisles <- names(aisle_counts[aisle_counts > 10000])
```

```{r}
# Make a plot that shows the number of items ordered in each aisle
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  arrange(desc(n)) %>%
  mutate(aisle = fct_reorder(aisle, n)) -> filtered_data

aisle_order_plot <- ggplot(filtered_data, aes(x = aisle, y = n)) +
  geom_bar(stat = "identity") +
  coord_flip() +
  labs(x = "Aisle", y = "Number of Items Ordered") +
  theme_minimal()

print(aisle_order_plot)
```

We can see the arrangement of these data from high to low, and we can see that there are 39 categories with more than 10,000 quantities, and fresh vegetable has the largest number. Butter accounts for the smallest proportion here.

```{r, message=FALSE}
# Make a table showing the three most popular items
three_mpi <- instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits"))

# Group aisle and product name
aisle_product_counts <- three_mpi %>%
  group_by(aisle, product_name) %>%
  summarise(order_count = n())%>%
  arrange(aisle, desc(order_count))

# Rank the items within each aisle by order count
aisle_product_counts <- aisle_product_counts %>%
  group_by(aisle) %>%
  mutate(rank = rank(-order_count))
top_items <- aisle_product_counts %>%
  filter(rank <= 3)
top_items <- top_items %>%
  select(aisle, rank, product_name, order_count)

print(top_items)
```

We selected the top three comprehensive rankings from these three categories, so that we can read this table more intuitively. And we can see that the number of vegetable orders is the highest, far exceeding the number of dog food care.

```{r, message=FALSE, warning=FALSE}
hour_day <- instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream"))%>%

# Calculate the mean hour of the day for each product on each day of the week
group_by(product_name, order_dow) %>%
summarise(mean_hour = mean(order_hour_of_day, na.rm = TRUE)) %>%
pivot_wider(names_from = product_name, values_from = mean_hour)%>%
  mutate(order_dow = factor(order_dow,
                            labels = c("Sunday", "Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday")))
print(hour_day)
```

From the data we compiled, we can see that for coffee ice cream, Tuesday has the largest average value and Friday has the smallest average value. On the other hand, pink lady apple has the largest average on Tuesday and the smallest on Sunday.

#Problem 2
```{r}
library(p8105.datasets)
data("brfss_smart2010")
view(brfss_smart2010)
```

```{r}
# Data cleaning 
brfss <- brfss_smart2010 %>%
  janitor::clean_names() %>%
  filter(topic == "Overall Health") %>%
  mutate(response = factor(response, levels = c("Excellent", "Very good", 
                                                "Good", "Fair", "Poor")))
# organize responses as a factor taking levels ordered from “Poor” to “Excellent”

view(brfss)
```

```{r}
# states were observed at 7 or more locations at 2002
states_obv <- brfss%>%
  filter(year == "2002") %>%
  group_by(locationabbr)%>%
  summarise(count = n_distinct(locationdesc))%>%
filter(count >= 7)
print(states_obv)
num_rol <- nrow(states_obv)
```

From the result we can see that total of `r num_rol` states is present in the result. They are CT, FL, MA, NC, NJ, and PA are the states observed at 7 or more locations at the year of 2002. 

```{r}
# states were observed at 7 or more locations at 2010
states_obv <- brfss%>%
  filter(year == "2010") %>%
  group_by(locationabbr)%>%
  summarise(count = n_distinct(locationdesc))%>%
filter(count >= 7)
print(states_obv)
num_x <- nrow(states_obv)
```
From the result we can see that total of `r num_x` states is present in the result. They are CA, CO, FL, MA, MD, NC, NE, NJ, NY, and OH are the states observed at 7 or more locations at the year of 2010. 

```{r, message=FALSE}
# dataset that limited to Excellent responses and averages the data_value across locations
limdata <- brfss %>%
  filter(response == "Excellent")%>%
  group_by(year, locationabbr) %>%
  summarize(average_data_value = mean(data_value, na.rm = TRUE))

# Draw the graph 
spaghetti_plot <- ggplot(limdata, aes(x = year, y = average_data_value, 
                             group = locationabbr, color = locationabbr)) +
  geom_line() +
  labs(x = "Year", y = "Average Crude Score", color = "State") +
  theme_minimal() +
  theme(legend.position = "top")
print(spaghetti_plot)
```

As the graph shows, we can see that this graph is very complicated and cannot clearly see the gaps and data distribution between each state. But we can see that from 2002 to 2006, the average value of one state dropped significantly. Beyond that, the average score across states always seems to fall between 17 and 27, which is fairly evenly distributed over the years.

```{r}
ny_state <- brfss%>%
  filter(locationabbr == "NY", year %in% c(2006, 2010))%>%
  group_by(year)%>%

  ggplot(aes(x = response, y = data_value, fill = response)) +
  geom_boxplot() +
  facet_wrap(year ~ .) +
  labs(title = "Distribution of Data by Response in NY (2006 and 2010)",
       x = "Response Type", y = "Distribution of Crude Prevalence") +
  theme_minimal() 
print(ny_state)
```

From the graph we can see that, for "Excellent", the data in 2006 and 2010 are relatively stable. The biggest difference is the "Good". Although the median in 2010 has not changed much compared to 2006, the overall data range is lower than that in 2006 a lot. For "Fair" people, the range in 2010 is obviously larger than the range in 2006. Therefore, the prevalence score is somehow positively or have related to the response type.

# Problem 3
```{r}
# Load, tidy and organize the acceleration data sets
acc_data <- read_csv("./data/nhanes_accel.csv") %>%
janitor::clean_names()

# Load, tidy and organize the covar data sets.
covar <- read_csv("./data/nhanes_covar.csv", skip = 4) %>%
  janitor::clean_names()%>%
  mutate(education = recode(education, "1" = "Less than high school", 
                          "2" = "High school equivalent", "3" = "More than high school"))
covar$sex <- ifelse(covar$sex == 1, "Male", "Female") 
covar <- covar %>%
  mutate(sex = factor(sex), education = factor(education))

# exclude age less than 21 yrs old
covar <- covar %>%
  filter(age >= 21) %>%
drop_na()

# Merge both data set
combined <- left_join(covar, acc_data, by = "seqn")
view(combined)
```

This combined data table, as required, contains all information from both data sets and excludes persons under the age of 21, also bind with the same "seqn". Not only that, but also sent out those NA data.

```{r, message=FALSE}
# Table for men and women with different education level
combined <- combined %>%
  filter(!is.na(sex) & !is.na(education))
education_table <- combined %>%
  group_by(education, sex) %>%
  summarise(count = n()) %>%
  pivot_wider(names_from = sex, values_from = count) %>%
  mutate(Total = Male + Female) %>%
  arrange(desc(Total))
view(education_table)
```

```{r, message=FALSE}
# visualization of the age distributions for men and women in each education category
vis_plot <- ggplot(combined, aes(x = education, y = age, fill = sex)) +
  geom_boxplot() +
  labs(title = "Age Distribution by Gender and Education",
       x = "Education Level",
       y = "Age") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
print(vis_plot)
```

Overall, this visualization effectively conveys how age, education, and gender are related to each other in our dataset. For "high school equivalent", the average data of women is significantly higher than that of men, and there is not much difference in the other two intervals. For "More than high school", the median value for women is lower than that for men. And from this data, we can also see that young people account for a larger proportion of those with higher education.

```{r}
# compare men to women and have separate panels for each education level with trend line
new_combined <- combined %>%
  mutate(total_activity = rowSums(across(min1:min1440), na.rm = TRUE))
activity_plot <- ggplot(new_combined, aes(x = age, y = total_activity, color = sex)) +
  geom_point() +
  facet_wrap(~education, scales = "free") +  
  geom_smooth(method = "lm", se = FALSE, formula = y ~ x) +  
  labs(title = "Total Activity by Age, Gender, and Education",
       x = "Age",
       y = "Total Activity",
       color = "Gender") +
  theme_minimal()
print(activity_plot)
```

From these three pictures, we can clearly see that there are Outliers in all three groups, but the number of Outliers in "more than high school" is the smallest. And as age increases, the number of total activities decreases significantly. And in the "less than high school" group, the rate of decline is most obvious. The slope is gentlest in "more than high school."

```{r}
# 24-hour activity time courses for each education level and use color to indicate sex
activity_data <- new_combined %>%
  group_by(education, sex) %>%
  summarise(across(starts_with("min"), ~ mean(.), .names = "mean_{.col}"), .groups = "drop") %>%
  pivot_longer(cols = starts_with("mean_"), names_to = "time", values_to = "mean") %>%
  mutate(
    time = substring(time, 9),
    time = as.numeric(time) / 60
  )

ggplot(activity_data, aes(x = time, y = mean, color = sex)) +
  geom_line() +
  facet_wrap(education ~ ., labeller = labeller(education = as_labeller(education_table))) +
  labs(title = "24-Hour Activity by Education and Gender",
       x = "Time (hours)",
       y = "Mean value of activity",
       color = "Gender") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red"), name = "Gender")
```
From these three pictures, we can see that men’s general activity hours are lower than women in three different groups in the 10-20 hour range. However, in less than 5 hours of exercise, the value of men is slightly higher than that of women. Not only that, the distribution forms of the three pictures are very similar, which shows that different education levels do not have a great impact on the length of exercise time. People of different genders and education levels are more likely to exercise for an average of 10-20 hours.




